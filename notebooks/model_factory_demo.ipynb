{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Factory Demo\n",
    "\n",
    "This notebook demonstrates the usage of the ModelFactory for Titanic survival prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Add the project root to path so we can import modules\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.data_loader import DataLoader\n",
    "from src.data_processing.data_preprocessor import DataPreprocessor\n",
    "from src.modelling.model_factory import ModelFactory\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = DataLoader()\n",
    "train_data = data_loader.load_train_data()\n",
    "test_data = data_loader.load_test_data()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Preview the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Get combined data for consistent preprocessing\n",
    "combined_data, n_train_samples = data_loader.get_combined_data()\n",
    "\n",
    "# Fit the preprocessor on the combined data\n",
    "processed_data = preprocessor.fit_transform(combined_data)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train = processed_data[:n_train_samples].drop('Survived', axis=1)\n",
    "y_train = processed_data[:n_train_samples]['Survived']\n",
    "X_test = processed_data[n_train_samples:]\n",
    "\n",
    "# Further split training data for model evaluation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available models\n",
    "available_models = ModelFactory.get_available_models()\n",
    "print(f\"Available models: {available_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default parameters for a model\n",
    "for model_type in available_models:\n",
    "    params = ModelFactory.get_model_default_params(model_type)\n",
    "    print(f\"\\nDefault parameters for {model_type}:\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate a Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "log_reg_model = ModelFactory.create_model('logistic_regression')\n",
    "\n",
    "# Train the model\n",
    "log_reg_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Evaluate on validation set\n",
    "metrics = log_reg_model.evaluate(X_val, y_val)\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = log_reg_model.get_feature_importance(feature_names=X_train.columns)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='coefficient', y='feature', data=feature_importance.head(15), palette='viridis')\n",
    "plt.title('Logistic Regression Feature Importance (Top 15)')\n",
    "plt.xlabel('Coefficient Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Compare Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and their metrics\n",
    "models = {}\n",
    "model_metrics = {}\n",
    "model_predictions = {}\n",
    "model_probabilities = {}\n",
    "\n",
    "# Train and evaluate all models\n",
    "for model_type in available_models:\n",
    "    print(f\"Training {model_type}...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = ModelFactory.create_model(model_type)\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "    models[model_type] = model\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = model.evaluate(X_val, y_val)\n",
    "    model_metrics[model_type] = metrics\n",
    "    \n",
    "    # Store predictions and probabilities\n",
    "    model_predictions[model_type] = model.predict(X_val)\n",
    "    model_probabilities[model_type] = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  ROC AUC: {metrics['roc_auc']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison dataframe\n",
    "comparison_data = []\n",
    "for model_type, metrics in model_metrics.items():\n",
    "    row = {'model': model_type}\n",
    "    row.update(metrics)\n",
    "    comparison_data.append(row)\n",
    "    \n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display the comparison\n",
    "comparison_df = comparison_df.sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "colors = sns.color_palette('viridis', len(metrics_to_plot))\n",
    "\n",
    "bar_width = 0.15\n",
    "index = np.arange(len(comparison_df))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(index + i * bar_width, comparison_df[metric], bar_width, \n",
    "            label=metric.replace('_', ' ').title(), color=colors[i])\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(index + bar_width * 2, comparison_df['model'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_type in model_probabilities.keys():\n",
    "    fpr, tpr, _ = roc_curve(y_val, model_probabilities[model_type])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_type} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model (based on ROC AUC) for hyperparameter tuning\n",
    "best_model_type = comparison_df.sort_values('roc_auc', ascending=False).iloc[0]['model']\n",
    "print(f\"Tuning hyperparameters for {best_model_type}...\")\n",
    "\n",
    "# Create a fresh model\n",
    "best_model = ModelFactory.create_model(best_model_type)\n",
    "\n",
    "# Get default parameter grid\n",
    "param_grid = best_model.get_param_grid()\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "\n",
    "# Tune hyperparameters (may take some time)\n",
    "best_model.tune_hyperparameters(X_train_final, y_train_final, cv=3, scoring='roc_auc')\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_metrics = best_model.evaluate(X_val, y_val)\n",
    "print(\"\\nTuned Model Metrics:\")\n",
    "for metric, value in tuned_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned model with original models\n",
    "original_metrics = model_metrics[best_model_type]\n",
    "metric_comparison = pd.DataFrame({\n",
    "    'Original': pd.Series(original_metrics),\n",
    "    'Tuned': pd.Series(tuned_metrics)\n",
    "})\n",
    "\n",
    "print(f\"Performance comparison for {best_model_type}:\")\n",
    "metric_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Selection and Kaggle Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model (tuned or from the original set)\n",
    "if tuned_metrics['roc_auc'] > original_metrics['roc_auc']:\n",
    "    final_model = best_model\n",
    "    print(f\"Selected the tuned {best_model_type} as the final model\")\n",
    "else:\n",
    "    # Find the best original model\n",
    "    best_original_type = comparison_df.iloc[0]['model']\n",
    "    final_model = models[best_original_type]\n",
    "    print(f\"Selected the original {best_original_type} as the final model\")\n",
    "\n",
    "# Train the final model on the full training set\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create a Kaggle submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': test_predictions.astype(int)\n",
    "})\n",
    "\n",
    "# Save the submission\n",
    "submission_path = '../kaggle_submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the final model\n",
    "feature_importance = final_model.get_feature_importance(feature_names=X_train.columns)\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Determine column to use based on the model type\n",
    "    if 'importance' in feature_importance.columns:\n",
    "        value_col = 'importance'\n",
    "        title = 'Feature Importance'\n",
    "    else:\n",
    "        value_col = 'coefficient'\n",
    "        feature_importance = feature_importance.copy()\n",
    "        feature_importance[value_col] = abs(feature_importance[value_col])  # Use absolute values for coefficients\n",
    "        title = 'Feature Coefficient Magnitude'\n",
    "        \n",
    "    # Sort and plot the top 20 features\n",
    "    top_features = feature_importance.sort_values(value_col, ascending=False).head(20)\n",
    "    sns.barplot(x=value_col, y='feature', data=top_features, palette='viridis')\n",
    "    plt.title(f'Top 20 Features by {title}')\n",
    "    plt.xlabel(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the full table\n",
    "    feature_importance.sort_values(value_col, ascending=False).head(20)\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the use of the ModelFactory to create, train, and evaluate different machine learning models for Titanic survival prediction. We:\n",
    "\n",
    "1. Created and compared multiple model types\n",
    "2. Visualized model performance metrics\n",
    "3. Performed hyperparameter tuning on the best model\n",
    "4. Generated a Kaggle submission file\n",
    "5. Analyzed feature importance\n",
    "\n",
    "The ModelFactory provides a standardized interface for working with different models, making it easy to experiment with various algorithms and find the best performing model for our task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
