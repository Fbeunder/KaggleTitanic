# Titanic Survival Prediction

![Titanic Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/800px-RMS_Titanic_3.jpg)

## ğŸ“‹ Project Overview

The Titanic Survival Prediction project is a machine learning application that analyzes passenger data from the historic Titanic disaster to predict survival outcomes. Based on the famous Kaggle competition dataset, this project demonstrates a complete machine learning workflow including:

1. Data processing and exploratory analysis of the Titanic passenger dataset
2. Feature engineering and selection to improve model performance
3. Training and evaluation of various machine learning models
4. A web interface for data exploration, model comparison, and making predictions
5. Generation of submission files for the Kaggle competition

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Virtual environment (recommended, but optional)

### Installation

1. Clone this repository:
```bash
git clone https://github.com/Fbeunder/KaggleTitanic.git
cd KaggleTitanic
```

2. Create and activate a virtual environment (optional but recommended):
```bash
# For macOS/Linux:
python -m venv venv
source venv/bin/activate

# For Windows:
python -m venv venv
venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Download the Titanic dataset from [Kaggle](https://www.kaggle.com/c/titanic/data) and place the files in the root directory or in the `data/` directory:
   - train.csv
   - test.csv

### Configuration

The application uses a configuration file that can be customized to adjust paths and settings. Configuration is managed through the `src/utilities/config.py` module.

## ğŸ—ï¸ Project Structure

```
KaggleTitanic/
â”œâ”€â”€ data/                   # Data directory for Titanic datasets
â”œâ”€â”€ notebooks/              # Jupyter notebooks for exploration and analysis
â”‚   â”œâ”€â”€ titanic_eda.ipynb        # Exploratory Data Analysis notebook
â”‚   â””â”€â”€ model_factory_demo.ipynb # Model Factory demonstration
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ data_processing/        # Data loading and preprocessing modules
â”‚   â”‚   â”œâ”€â”€ data_loader.py         # Loading train and test datasets
â”‚   â”‚   â””â”€â”€ data_preprocessor.py   # Data cleaning and transformation
â”‚   â”œâ”€â”€ feature_engineering/     # Feature creation and selection modules
â”‚   â”‚   â”œâ”€â”€ feature_creator.py     # Feature creation and transformation
â”‚   â”‚   â””â”€â”€ feature_selector.py    # Feature selection and importance analysis
â”‚   â”œâ”€â”€ modelling/               # Model training and evaluation modules
â”‚   â”‚   â”œâ”€â”€ model_factory.py       # Factory for creating ML models
â”‚   â”‚   â”œâ”€â”€ model_trainer.py       # Training and tuning models
â”‚   â”‚   â””â”€â”€ model_evaluator.py     # Evaluating model performance
â”‚   â”œâ”€â”€ utilities/               # Utility functions and configuration
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration settings
â”‚   â”‚   â””â”€â”€ utils.py              # Helper functions
â”‚   â””â”€â”€ web_interface/           # Web application interface
â”‚       â”œâ”€â”€ app.py               # Main Flask application
â”‚       â”œâ”€â”€ model_interface.py   # Interface between web app and models
â”‚       â”œâ”€â”€ dashboard.py         # Visualization dashboard
â”‚       â”œâ”€â”€ templates/           # HTML templates
â”‚       â””â”€â”€ static/              # CSS, JavaScript, and images
â”œâ”€â”€ tests/                  # Test modules
â”œâ”€â”€ submissions/            # Generated Kaggle submission files
â”œâ”€â”€ requirements.txt        # Project dependencies
â””â”€â”€ README.md               # Project documentation
```

## ğŸ§° Usage

### Running the Web Interface

To start the web application:

```bash
python -m src.web_interface.app
```

Then open your browser and go to: http://localhost:8050

### Web Interface Features

The web interface provides the following functionality:

1. **Data Exploration:** Visualize and explore the Titanic dataset
2. **Feature Engineering:** Select and create features for model training
3. **Model Training:** Train different models with customizable parameters
4. **Predictions:** Make predictions on the test set or custom input
5. **Kaggle Submissions:** Generate and download submission files

### Training Models Programmatically

You can also train models programmatically:

```python
from src.data_processing.data_loader import DataLoader
from src.data_processing.data_preprocessor import DataPreprocessor
from src.modelling.model_factory import ModelFactory
from src.modelling.model_trainer import ModelTrainer

# Load and preprocess data
loader = DataLoader()
train_data = loader.load_train_data()
preprocessor = DataPreprocessor()
X, y = preprocessor.preprocess(train_data)

# Create and train a model
factory = ModelFactory()
model = factory.create_model('random_forest')
trainer = ModelTrainer()
trainer.train(model, X, y)

# Make predictions
predictions = model.predict(X_test)
```

### Running Tests

To run tests:

```bash
python -m pytest tests/
```

## ğŸ“Š Project Components

### Data Processing

- **DataLoader:** Handles loading the training and testing datasets
- **DataPreprocessor:** Cleans, transforms, and prepares data for modeling

### Feature Engineering

- **FeatureCreator:** Creates new features from existing data
- **FeatureSelector:** Selects the most important features for model training

### Modeling

- **ModelFactory:** Creates different types of machine learning models
- **ModelTrainer:** Trains models with cross-validation and hyperparameter tuning
- **ModelEvaluator:** Evaluates model performance with metrics and visualizations

### Web Interface

- **App:** Main web application using Dash/Flask
- **ModelInterface:** Connects the web interface with the machine learning components
- **Dashboard:** Creates visualizations for the web interface

## ğŸ“ Example Workflows

### Basic Workflow

1. Load and preprocess the data
2. Engineer features
3. Train multiple models
4. Evaluate model performance
5. Generate predictions and submission file

### Advanced Workflow

1. Load and preprocess the data
2. Perform feature engineering with feature importance analysis
3. Use GridSearchCV for hyperparameter tuning
4. Train ensemble models
5. Evaluate with cross-validation
6. Generate predictions with confidence intervals

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Contribution Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## âœ‰ï¸ Contact

For questions or feedback about this project, please open an issue on GitHub.
